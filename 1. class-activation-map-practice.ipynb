{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. class-activation-map-practice.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"A0K3rq2AhvJH","colab_type":"text"},"source":["# CAM: Class Activation Map\n","\n","\n","<center>\n","<img src=\"https://i.imgur.com/ZbuAN3A.png\" width=\"400\">\n","</center>\n","\n","In this work, we revisit the **global average pooling** layer proposed, and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on image level labels. \n","\n","\n","\n","\n","**Global Average Pooling for localizable deep representation**\n","\n","While this technique was previously proposed\n","as a means for regularizing training, researchers find that it actually builds a generic localizable deep representation that\n","exposes the implicit attention of CNNs on an image.\n","\n","<center/>\n","<img src=\"https://you359.github.io/images/contents/cam_gap.png\" width=400>\n","</center>\n","\n","\n","\n","**How GAP represent localizable representation?**\n","\n","Let's compare [flatten] + [fully connected layerS] and [global average pooling] +  [one fully connected layer].\n","\n","1. [flatten] + [fully connected layerS]\n","\n","    <center/>\n","    <img src=\"https://www.researchgate.net/profile/Budiman_Minasny/publication/334783857/figure/fig4/AS:786596169269249@1564550549811/Illustration-of-flatten-layer-that-is-connecting-the-pooling-layers-to-the-fully.png\" width=300>\n","    </center>\n","\n","\n","2. [global average pooling] +  [one fully connected layer]\n","\n","    It can contain localizable representation.\n","    <center/>\n","    <img src=\"https://you359.github.io/images/contents/cam_gap.png\" width=300>\n","    </center>\n","    <center/>\n","    <img src=\"https://jsideas.net/assets/materials/20180104/S_c.png\" width=300>\n","    </center>\n","\n","\n","Here is a **[paper link.](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XZVMslRA1GW5","colab_type":"text"},"source":["# Import module\n"]},{"cell_type":"code","metadata":{"id":"1Gzt0bJphov2","colab_type":"code","colab":{}},"source":["# import module\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","\n","import torch.nn.functional as F\n","from skimage.transform import resize\n","import matplotlib.pyplot as plt\n","%matplotlib inline  \n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hrRMgSguh5J","colab_type":"text"},"source":["# Hyperparameters\n","\n"]},{"cell_type":"code","metadata":{"id":"DLQ3dFBguhZ9","colab_type":"code","colab":{}},"source":["batch_size = 32\n","learning_rate = 0.001\n","num_epoch = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Y90lRWCwDJ8","colab_type":"text"},"source":["# Data load: MNIST\n"]},{"cell_type":"code","metadata":{"id":"3LXuckN1xSrQ","colab_type":"code","colab":{}},"source":["mnist_train = dset.MNIST(\"./\", train=True, \n","                         transform=transforms.Compose([\n","                            transforms.RandomCrop(22),\n","                            transforms.Resize(226),\n","                            transforms.ToTensor(),\n","                         ]), \n","                         target_transform=None, \n","                         download=True)\n","\n","mnist_test = dset.MNIST(\"./\", train=False,\n","                        transform=transforms.Compose([\n","                            transforms.RandomCrop(22),\n","                            transforms.Resize(226),\n","                            transforms.ToTensor(),\n","                        ]),\n","                        target_transform=None, \n","                        download=True)\n","\n","train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True)\n","test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3Rl-zwNyFht","colab_type":"text"},"source":["# Model: Resnet34\n","\n","Resnet use global average pooling. Please check this image.\n","\n","\n","<center/>\n","<img src=\"https://i.stack.imgur.com/ElFiI.png\" width=250>\n","</center>\n"]},{"cell_type":"code","metadata":{"id":"9Dctbl9RyEGs","colab_type":"code","colab":{}},"source":["import torchvision.models as models\n","resnet34 = models.resnet34(pretrained=True)\n","resnet34.fc = nn.Linear(resnet34.fc.in_features, 10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpePpfNb_LIl","colab_type":"text"},"source":["## Train\n"]},{"cell_type":"code","metadata":{"id":"6n9bK1wW_Kvf","colab_type":"code","outputId":"38a2274a-f8c5-49b4-89f6-564ab657843a","executionInfo":{"status":"ok","timestamp":1579859165725,"user_tz":-540,"elapsed":368166,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["loss_func = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(resnet34.parameters(), lr=learning_rate)\n","resnet34.cuda()\n","for i in range(num_epoch):\n","    resnet34.train()\n","    for j,[image,label] in enumerate(train_loader):\n","        x = Variable(image)\n","        x = torch.cat([x, x, x], dim=1).type(torch.FloatTensor).cuda()\n","        y_= Variable(label).cuda()\n","        optimizer.zero_grad()\n","        output = resnet34.forward(x)\n","        loss = loss_func(output,y_)\n","        loss.backward()\n","        optimizer.step()\n","        \n","    top_1_count = torch.FloatTensor([0])\n","    total = torch.FloatTensor([0])\n","    resnet34.eval() \n","    for image,label in test_loader:\n","        x = Variable(image)\n","        x = torch.cat([x, x, x], dim=1).type(torch.FloatTensor).cuda()\n","        y_= Variable(label).cuda()\n","        output = resnet34.forward(x)\n","        values,idx = output.max(dim=1)\n","        top_1_count += torch.sum(y_==idx).float().cpu().data\n","\n","        total += label.size(0)\n","\n","    print(\"Test Data Accuracy: {}%\".format(100*(top_1_count/total).numpy()))\n","    if (top_1_count/total).numpy() > 0.98:\n","        break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Data Accuracy: [98.97836]%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3eZOvX6n3fGD","colab_type":"text"},"source":["# Class Activation Map module\n","\n","Class activation map is computed by weight sum of feature-maps. \n","\n","It means that we compute a\n","weighted sum of the feature maps of **the last convolutional**\n","layer to obtain our class activation maps.\n","\n","$$\n","W_1^c \\cdot F_1^c + W_2^c \\cdot F_2^c + \\cdots + W_N^c \\cdot F_N^c = CAM_c\n","$$\n","\n","\n","- $W_k$: importance of F_k, weight value for predicting the input image as class $c$.\n","- $F_k$: Feature map k-th\n","- $c$: class index"]},{"cell_type":"code","metadata":{"id":"8FJNK6Z23ZiR","colab_type":"code","colab":{}},"source":["from torchvision import transforms\n","from torch.autograd import Variable\n","from torch.nn import functional as F\n","import numpy as np\n","import cv2, torch\n","\n","# generate class activation mapping for the top1 prediction\n","def returnCAM(feature_conv, weight_softmax, class_idx):\n","    # generate the class activation maps upsample to 256x256\n","    bz, nc, h, w = feature_conv.shape\n","    output_cam = []\n","    cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w))) # weight_softmax: shape 1 * 512 featureConv: shape 512 * h * w\n","    cam = np.reshape(cam, (w, h))\n","    cam = cam - np.min(cam)\n","    cam_img = cam / np.max(cam)\n","    cam_img = np.uint8(255 * cam_img)\n","    return cam_img\n","\n","def get_cam(net, features_blobs, img, classes):\n","    params = list(net.parameters())\n","    weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n","    img_pil = torch.cat([img, img, img], dim=0)\n","    img_pil = transforms.ToPILImage()(img_pil.type(torch.FloatTensor))\n","   \n","    normalize = transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","    preprocess = transforms.Compose([\n","        transforms.Resize((226, 226)),\n","        transforms.ToTensor(),\n","        normalize\n","    ])\n","    \n","    img_tensor = preprocess(img_pil)\n","    img_variable = Variable(img_tensor.unsqueeze(0)).cuda()\n","    logit = net(img_variable)\n","    h_x = F.softmax(logit, dim=1).data.squeeze()\n","    probs, idx = h_x.sort(0, True)\n","\n","    # output: the prediction\n","    for i in range(0, 10):\n","        line = '{:.3f} -> {}'.format(probs[i], classes[idx[i].item()])\n","        print(line)\n","\n","    CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0].item()])\n","\n","    # render the CAM and output\n","    print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0].item()])\n","    _, height, width = img.shape\n","    img = img.detach().cpu().numpy()\n","    img = np.squeeze(img)\n","    \n","    CAM = cv2.resize(CAMs, (width, height))\n","    result = img * 255 * 0.5 \n","\n","    plt.imshow(result, cmap=\"gray\")\n","    plt.imshow(CAM, alpha=0.5, cmap='hot')\n","    plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"94WwcbipHRfI","colab_type":"code","outputId":"f4ac027c-5be0-45e1-add4-36b86be2a70c","executionInfo":{"status":"ok","timestamp":1579859769299,"user_tz":-540,"elapsed":806,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# hook the feature extractor\n","features_blobs = []\n","\n","def hook_feature(module, input, output):\n","    features_blobs.append(output.data.cpu().numpy())\n","\n","resnet34.layer4[-1].conv2.register_forward_hook(hook_feature)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.hooks.RemovableHandle at 0x7f1799cd3320>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"vUayFknoz3jJ","colab_type":"code","colab":{}},"source":["classes = {0: '1', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8:'8', 9:'9'}\n","features_blobs = []\n","resnet34.cuda()\n","for image, label in mnist_train:\n","    get_cam(resnet34, features_blobs, image[0].unsqueeze(0), classes)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJeNn0ivpsDx","colab_type":"text"},"source":["## Reproduce!"]},{"cell_type":"code","metadata":{"id":"5BvsKSRnpxp3","colab_type":"code","colab":{}},"source":["class CAM:\n","    \"\"\"Class Activation Map for resnet34\"\"\"\n","\n","    def __init__(self, model: torch.nn.module, weight: np.ndarray):\n","        self.model = model\n","        self.params = list(self.model.parameters())\n","        self.weight_softmax = np.squeeze(self.params[-2].data.cpu().numpy())\n","        self.features_blobs = []\n","\n","        # last conv\n","        self.model.layer4[-1].conv2.register_forward_hook(self._hook_forward())\n","\n","    def _hook_forward(self):\n","        def hook_feature(module, input, output):\n","            self.features_blobs.append(output.data.cpu().numpy())\n","        return hook_feature\n","\n","\n","    def get_cam(self, img:np.ndarray, target:int=None):\n","        _, height, width = img.shape\n","        input_tensor = self._make_tensor(img)\n","        logit = self.model(output)\n","        softmax, idx = logit.sort(0, True)\n","        CAM = self.calculate_cam(self.features_blobs[-1], self.weight_softmax,idx[0].item())\n","        return CAM\n","\n","    def calculate_cam(self,feature_map, weight, class_idx):\n","        bz, nc, h, w = feature_map.shape\n","        output_cam = []\n","        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w))) # weight_softmax: shape 1 * 512 featureConv: shape 512 * h * w\n","        cam = np.reshape(cam, (w, h))\n","        cam = cam - np.min(cam)\n","        cam = cam / np.max(cam)\n","        cam = np.uint8(255 * cam)\n","        return cam\n","\n","     def _make_tensor(self, img:np.ndarray):\n","        img_pil = torch.cat([img, img, img], dim=0)\n","        img_pil = transforms.ToPILImage()(img_pil.type(torch.FloatTensor))\n","    \n","        normalize = transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        )\n","        preprocess = transforms.Compose([\n","            transforms.Resize((226, 226)),\n","            transforms.ToTensor(),\n","            normalize\n","        ])\n","        \n","        img_tensor = preprocess(img_pil)\n","        img_variable = Variable(img_tensor.unsqueeze(0)).cuda()\n","\n","        h_x = F.softmax(logit, dim=1).data.squeeze()\n","        probs, idx = h_x.sort(0, True)\n","        return img_variable\n","    \n","     \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8zL_lhidhmNQ","colab_type":"text"},"source":[""]}]}