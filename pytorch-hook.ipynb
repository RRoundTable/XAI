{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch-hook.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V6s4l84ijgm1","colab_type":"text"},"source":["# **Pytorch Hook**\n","\n","Hook provide you to do things during backpropagation.\n","\n","You can register a hook on a Tensor or a nn.Module. \n","\n","You can register a hook on a Tensor or a nn.Module. A hook is basically a function that is executed when the either forward or backward is called.\n","\n","\n","When I say forward, I don't mean the forward of a nn.Module . forward function here means the forward  function of the **torch.Autograd.Function** object that is the grad_fn of a Tensor.\n","\n","1.   Forward Hook\n","2.   Backward Hook\n","\n","### Reference\n","- [computation graph in pytorch](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)\n","\n","- [Pytorch-Hook](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LRxaQ0uHPfoB","colab_type":"text"},"source":["## Hooks for Tensor\n","\n","There is no forward hook for a tensor.\n","\n","```python\n","# backward hook for tensor\n","hook(grad) -> Tensor or None\n","```\n","\n","grad is basically the value contained in the grad attribute of the tensor after backward is called\n"]},{"cell_type":"code","metadata":{"id":"TQUMCHWjPjd5","colab_type":"code","colab":{}},"source":["import torch \n","a = torch.ones(5)\n","a.requires_grad = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_KJLQuqNQNV3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"445b2542-4967-43b5-8b18-351337ce53a5","executionInfo":{"status":"ok","timestamp":1577364572858,"user_tz":-540,"elapsed":388,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["# Tensor user make has no grad_fn.\n","print(a)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ye2wQqTDQNYw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d0651cd5-e15c-479e-a1e7-69b7f8c4686b","executionInfo":{"status":"ok","timestamp":1577364590299,"user_tz":-540,"elapsed":659,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["b = 2*a\n","print(b)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ekwnuyZKkFCg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bf65c681-bab5-4c0f-d84c-7d6145891cea","executionInfo":{"status":"ok","timestamp":1577364709819,"user_tz":-540,"elapsed":635,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["b.retain_grad()  # Since b is non-leaf and it's grad will be destroyed otherwise\n","print(b)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dURcKHxFQ0_V","colab_type":"code","colab":{}},"source":["# https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944\n","# backward computes dvalue/dx\n","\n","c = b.mean()\n","c.backward() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wGq2sVpRQ1Bi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2f55a659-2f13-480d-b877-cc4d79ac6a3d","executionInfo":{"status":"ok","timestamp":1577364820069,"user_tz":-540,"elapsed":655,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["print(a.grad, b.grad)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t7wA0d0XRjj9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"7c27e51b-def0-4d69-81d8-05065e1c555c","executionInfo":{"status":"ok","timestamp":1577364960812,"user_tz":-540,"elapsed":655,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["# Redo with hook.\n","a = torch.ones(5)\n","\n","a.requires_grad = True\n","\n","b = 2*a\n","\n","b.retain_grad()\n","\n","b.register_hook(lambda x: print(x))  \n","\n","b.mean().backward() \n","print(a.grad, b.grad)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n","tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RYIaJv_sTDou","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"6c9c5722-26e5-4e92-e8cb-8a9f97ee34cc","executionInfo":{"status":"ok","timestamp":1577365293801,"user_tz":-540,"elapsed":880,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["# You can modify the gradient value.\n","a = torch.ones(5)\n","\n","a.requires_grad = True\n","b = 2*a\n","\n","b.retain_grad()\n","\n","\n","b.mean().backward() \n","\n","\n","print(a.grad, b.grad)\n","\n","b.grad *= 2\n","\n","print(a.grad, b.grad)       # a's gradient needs to updated manually"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n","tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nn93YrddRwkN","colab_type":"text"},"source":["## Hooks for nn.Module objects\n","\n","- Forward Hook\n","```python\n","hook(module, grad_input, grad_output) -> Tensor or None\n","```\n","\n","- Backward Hook\n","```python\n","hook(module, input, output) -> None\n","```\n","\n","- reference: https://tutorials.pytorch.kr/beginner/former_torchies/nn_tutorial.html\n"]},{"cell_type":"markdown","metadata":{"id":"fYkFTDeLTVUO","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"isL6G7bLTSzn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":316},"outputId":"289de323-b37e-4b43-f2e6-4df8608a47c8","executionInfo":{"status":"ok","timestamp":1577367205011,"user_tz":-540,"elapsed":748,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["import torch \n","import torch.nn as nn\n","\n","class myNet(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv = nn.Conv2d(3,10,2, stride = 2)\n","    self.relu = nn.ReLU()\n","    self.flatten = lambda x: x.view(-1)\n","    self.fc1 = nn.Linear(160,5)\n","   \n","  \n","  def forward(self, x):\n","    x = self.relu(self.conv(x))\n","    return self.fc1(self.flatten(x))\n","  \n","\n","net = myNet()\n","\n","def hook_fn(m, i, o):\n","  print(m)\n","  print(\"------------Input Grad------------\")\n","\n","  for grad in i:\n","    try:\n","      print(grad.shape)\n","    except AttributeError: \n","      print (\"None found for Gradient\")\n","\n","  print(\"------------Output Grad------------\")\n","  for grad in o:  \n","    try:\n","      print(grad.shape)\n","    except AttributeError: \n","      print (\"None found for Gradient\")\n","  print(\"\\n\")\n","net.conv.register_backward_hook(hook_fn)\n","net.fc1.register_backward_hook(hook_fn)\n","inp = torch.randn(1,3,8,8)\n","out = net(inp)\n","\n","(1 - out.mean()).backward()"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Linear(in_features=160, out_features=5, bias=True)\n","------------Input Grad------------\n","torch.Size([5])\n","torch.Size([5])\n","------------Output Grad------------\n","torch.Size([5])\n","\n","\n","Conv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))\n","------------Input Grad------------\n","None found for Gradient\n","torch.Size([10, 3, 2, 2])\n","torch.Size([10])\n","------------Output Grad------------\n","torch.Size([1, 10, 4, 4])\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IlZO1QZqauQ-","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class MNISTConvNet(nn.Module):\n","\n","    def __init__(self):\n","        # 여기에서모든 모듈을 초기화해놓고,\n","        # 나중에 여기에 선언한 이름으로 접근할 수 있습니다.\n","        super(MNISTConvNet, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, 5)\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(10, 20, 5)\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    # 순전파 함수에서 신경망의 구조를 정의합니다.\n","    # 여기에서는 단 하나의 입력만 받지만, 필요하면 더 받도록 변경하면 됩니다.\n","    def forward(self, input):\n","        x = self.pool1(F.relu(self.conv1(input)))\n","        x = self.pool2(F.relu(self.conv2(x)))\n","\n","        # 모델 구조를 정의할 때는 어떤 Python 코드를 사용해도 괜찮습니다.\n","        # 모든 코드는 autograd에 의해 올바르고 완벽하게 처리될 것입니다.\n","        # if x.gt(0) > x.numel() / 2:\n","        #      ...\n","        #\n","        # 심지어 동일한 모듈을 재사용하거나 반복(loop)해도 됩니다.\n","        # 모듈은 더 이상 일시적인 상태를 갖고 있지 않으므로,\n","        # 순전파 과정에서 여러번 사용해도 됩니다.\n","        # while x.norm(2) < 10:\n","        #    x = self.conv1(x)\n","\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return x\n","net = MNISTConvNet()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WkXZ4SuZv3z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":334},"outputId":"8afe9f1f-6361-4ff9-b54f-e269897fe5a6","executionInfo":{"status":"ok","timestamp":1577367360307,"user_tz":-540,"elapsed":591,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["def printnorm(self, input, output):\n","    # input is a tuple of packed inputs\n","    # output is a Tensor. output.data is the Tensor we are interested\n","    print('Inside ' + self.__class__.__name__ + ' forward')\n","    print('')\n","    print('input: ', type(input))\n","    print('input[0]: ', type(input[0]))\n","    print('output: ', type(output))\n","    print('')\n","    print('input size:', input[0].size())\n","    print('output size:', output.data.size())\n","    print('output norm:', output.data.norm())\n","\n","\n","net.conv2.register_forward_hook(printnorm)\n","inp = torch.randn(1, 1, 28, 28)\n","out = net(inp)"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Inside Conv2d forward\n","\n","input:  <class 'tuple'>\n","input[0]:  <class 'torch.Tensor'>\n","output:  <class 'torch.Tensor'>\n","\n","input size: torch.Size([1, 10, 12, 12])\n","output size: torch.Size([1, 20, 8, 8])\n","output norm: tensor(16.1047)\n","Inside Conv2d forward\n","\n","input:  <class 'tuple'>\n","input[0]:  <class 'torch.Tensor'>\n","output:  <class 'torch.Tensor'>\n","\n","input size: torch.Size([1, 10, 12, 12])\n","output size: torch.Size([1, 20, 8, 8])\n","output norm: tensor(16.1047)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m-M1XjaPZ630","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":369},"outputId":"18729de8-c0d8-408b-ea5a-e9dbf953e367","executionInfo":{"status":"ok","timestamp":1577367345017,"user_tz":-540,"elapsed":637,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["def printgradnorm(self, grad_input, grad_output):\n","    print('Inside ' + self.__class__.__name__ + ' backward')\n","    print('Inside class:' + self.__class__.__name__)\n","    print('')\n","    print('grad_input: ', type(grad_input))\n","    print('grad_input[0]: ', type(grad_input[0]))\n","    print('grad_output: ', type(grad_output))\n","    print('grad_output[0]: ', type(grad_output[0]))\n","    print('')\n","    print('grad_input size:', grad_input[0].size())\n","    print('grad_output size:', grad_output[0].size())\n","    print('grad_input norm:', grad_input[0].norm())\n","\n","\n","net.conv2.register_backward_hook(printgradnorm)\n","inp = torch.randn(1, 1, 28, 28)\n","out = net(inp)\n","(1 - out.mean()).backward()"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Inside Conv2d forward\n","\n","input:  <class 'tuple'>\n","input[0]:  <class 'torch.Tensor'>\n","output:  <class 'torch.Tensor'>\n","\n","input size: torch.Size([1, 10, 12, 12])\n","output size: torch.Size([1, 20, 8, 8])\n","output norm: tensor(17.0415)\n","Inside Conv2d backward\n","Inside class:Conv2d\n","\n","grad_input:  <class 'tuple'>\n","grad_input[0]:  <class 'torch.Tensor'>\n","grad_output:  <class 'tuple'>\n","grad_output[0]:  <class 'torch.Tensor'>\n","\n","grad_input size: torch.Size([1, 10, 12, 12])\n","grad_output size: torch.Size([1, 20, 8, 8])\n","grad_input norm: tensor(0.0147)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S8mRuKkTVno7","colab_type":"text"},"source":["## Proper Way of Using Hooks : An Opinion"]},{"cell_type":"code","metadata":{"id":"RFo0py_fTS2h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"53df607d-928f-44d2-cba7-922fc95ab5cb","executionInfo":{"status":"ok","timestamp":1577365971274,"user_tz":-540,"elapsed":738,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["import torch \n","import torch.nn as nn\n","\n","class myNet(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv = nn.Conv2d(3,10,2, stride = 2)\n","    self.relu = nn.ReLU()\n","    self.flatten = lambda x: x.view(-1)\n","    self.fc1 = nn.Linear(160,5)\n","   \n","  \n","  def forward(self, x):\n","    x = self.relu(self.conv(x))\n","    x.register_hook(lambda grad : torch.clamp(grad, min = 0))     #No gradient shall be backpropagated \n","                                                                  #conv outside less than 0\n","      \n","    # print whether there is any negative grad\n","    x.register_hook(lambda grad: print(\"Gradients less than zero:\", bool((grad < 0).any())))  \n","    return self.fc1(self.flatten(x))\n","  \n","\n","net = myNet()\n","\n","for name, param in net.named_parameters():\n","  # if the param is from a linear and is a bias\n","  if \"fc\" in name and \"bias\" in name:\n","    param.register_hook(lambda grad: torch.zeros(grad.shape))\n","\n","\n","out = net(torch.randn(1,3,8,8)) \n","\n","(1 - out).mean().backward()\n","\n","print(\"The biases are\", net.fc1.bias.grad)     #bias grads are zero"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Gradients less than zero: False\n","The biases are tensor([0., 0., 0., 0., 0.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"je4jpi0RWxNk","colab_type":"text"},"source":["## The Forward Hook for Visualising Activations"]},{"cell_type":"code","metadata":{"id":"_nlp4prpWvyM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"3f3af124-91ca-4c11-a599-4c9834052b73","executionInfo":{"status":"ok","timestamp":1577366315216,"user_tz":-540,"elapsed":630,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["import torch \n","import torch.nn as nn\n","\n","class myNet(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv = nn.Conv2d(3,10,2, stride = 2)\n","    self.relu = nn.ReLU()\n","    self.flatten = lambda x: x.view(-1)\n","    self.fc1 = nn.Linear(160,5)\n","    self.seq = nn.Sequential(nn.Linear(5,3), nn.Linear(3,2))\n","    \n","   \n","  \n","  def forward(self, x):\n","    x = self.relu(self.conv(x))\n","    x = self.fc1(self.flatten(x))\n","    x = self.seq(x)\n","  \n","\n","net = myNet()\n","visualisation = {}\n","\n","def hook_fn(m, i, o):\n","  visualisation[m] = o \n","\n","def get_all_layers(net):\n","  for name, layer in net._modules.items():\n","    #If it is a sequential, don't register a hook on it\n","    # but recursively register hook on all it's module children\n","    if isinstance(layer, nn.Sequential):\n","      get_all_layers(layer)\n","    else:\n","      # it's a non sequential. Register a hook\n","      layer.register_forward_hook(hook_fn)\n","\n","get_all_layers(net)\n","\n","  \n","out = net(torch.randn(1,3,8,8))\n","\n","# Just to check whether we got all layers\n","visualisation.keys()      #output includes sequential layers"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys([Conv2d(3, 10, kernel_size=(2, 2), stride=(2, 2)), ReLU(), Linear(in_features=160, out_features=5, bias=True), Linear(in_features=5, out_features=3, bias=True), Linear(in_features=3, out_features=2, bias=True)])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"1af3nTjCXPHL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3216e139-c1bc-46a8-cc11-0930df483ee1","executionInfo":{"status":"ok","timestamp":1577366455594,"user_tz":-540,"elapsed":619,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["list(visualisation.keys())[0]"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Conv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"RIwW-4vZXL0F","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"0pAaI0xiWv1y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":897},"outputId":"9a0d49dd-a36f-46ec-edb4-f02da1047e9d","executionInfo":{"status":"ok","timestamp":1577366459013,"user_tz":-540,"elapsed":593,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["visualisation[list(visualisation.keys())[0]]"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[-0.1303, -0.1141,  0.8475, -0.3684],\n","          [-1.0200,  0.2118,  0.9783,  0.5840],\n","          [ 0.3214,  0.6003, -0.2423,  0.3670],\n","          [ 0.7810, -1.3224,  0.5154, -0.5555]],\n","\n","         [[-0.3507, -0.1943, -0.1344, -0.2603],\n","          [-0.3106, -0.0644,  1.1350, -0.4445],\n","          [ 0.5473,  0.9649,  0.1628,  0.2881],\n","          [ 0.1003, -1.2433, -0.9339,  0.3527]],\n","\n","         [[ 0.6440,  0.7188, -0.6971,  0.4463],\n","          [ 0.5904,  0.8193, -0.2583, -0.7565],\n","          [-0.0084, -0.3424,  0.4223,  0.0085],\n","          [-0.2998,  0.0545, -0.9587,  0.7342]],\n","\n","         [[-0.1454,  0.5621,  1.0001,  0.7003],\n","          [-0.8641,  0.2516, -0.1495,  0.5826],\n","          [-0.0562, -0.9388,  0.5149, -0.1571],\n","          [ 0.8980, -0.0932,  0.9554, -0.8234]],\n","\n","         [[-0.5771, -0.2858, -0.1472, -0.6650],\n","          [-0.6099, -0.2352,  0.3240,  1.1149],\n","          [ 0.4402,  1.0028, -0.3964,  0.7477],\n","          [ 0.4653, -0.2095, -0.0076,  0.2770]],\n","\n","         [[-0.2883,  1.0039,  0.1484, -0.5896],\n","          [ 0.1415,  1.0619,  0.4815, -0.0905],\n","          [ 0.1933,  0.4732,  0.6349, -0.1135],\n","          [-0.6079,  0.1711,  0.1011,  0.0501]],\n","\n","         [[-0.7552, -0.3677,  0.2576,  0.0188],\n","          [-0.0208,  1.2025,  0.3414, -0.2160],\n","          [-0.0601,  0.0071,  0.2957, -0.1342],\n","          [ 0.4280, -0.7579,  0.0695, -0.6854]],\n","\n","         [[-0.0828, -1.0271, -0.2428,  0.3407],\n","          [ 0.3947, -0.9862,  0.5601, -0.7632],\n","          [ 0.4072, -0.0322, -0.2534,  0.3469],\n","          [ 0.1545, -0.7773, -0.4041,  0.0310]],\n","\n","         [[ 0.0088, -0.7376, -0.5655,  0.3615],\n","          [-0.2120,  0.1298, -0.3131, -0.3914],\n","          [ 0.5739,  0.0957, -0.4137,  0.7847],\n","          [ 0.5605, -0.1238, -0.4354,  0.1339]],\n","\n","         [[-0.2081, -0.1174, -0.0324, -0.2173],\n","          [-0.5697, -0.9581,  0.4856, -0.3257],\n","          [ 0.1740,  0.1962, -0.4146,  0.1627],\n","          [-0.2154, -1.3843, -0.1173, -0.6776]]]],\n","       grad_fn=<MkldnnConvolutionBackward>)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"Kdq8wWPPTS5T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":879},"outputId":"f79aaff4-b958-4203-c8f1-f4c99ef981a6","executionInfo":{"status":"ok","timestamp":1577366469304,"user_tz":-540,"elapsed":736,"user":{"displayName":"류원탁","photoUrl":"","userId":"13187082005098263686"}}},"source":["visualisation[list(visualisation.keys())[1]]"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[0.0000, 0.0000, 0.8475, 0.0000],\n","          [0.0000, 0.2118, 0.9783, 0.5840],\n","          [0.3214, 0.6003, 0.0000, 0.3670],\n","          [0.7810, 0.0000, 0.5154, 0.0000]],\n","\n","         [[0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 1.1350, 0.0000],\n","          [0.5473, 0.9649, 0.1628, 0.2881],\n","          [0.1003, 0.0000, 0.0000, 0.3527]],\n","\n","         [[0.6440, 0.7188, 0.0000, 0.4463],\n","          [0.5904, 0.8193, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.4223, 0.0085],\n","          [0.0000, 0.0545, 0.0000, 0.7342]],\n","\n","         [[0.0000, 0.5621, 1.0001, 0.7003],\n","          [0.0000, 0.2516, 0.0000, 0.5826],\n","          [0.0000, 0.0000, 0.5149, 0.0000],\n","          [0.8980, 0.0000, 0.9554, 0.0000]],\n","\n","         [[0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.3240, 1.1149],\n","          [0.4402, 1.0028, 0.0000, 0.7477],\n","          [0.4653, 0.0000, 0.0000, 0.2770]],\n","\n","         [[0.0000, 1.0039, 0.1484, 0.0000],\n","          [0.1415, 1.0619, 0.4815, 0.0000],\n","          [0.1933, 0.4732, 0.6349, 0.0000],\n","          [0.0000, 0.1711, 0.1011, 0.0501]],\n","\n","         [[0.0000, 0.0000, 0.2576, 0.0188],\n","          [0.0000, 1.2025, 0.3414, 0.0000],\n","          [0.0000, 0.0071, 0.2957, 0.0000],\n","          [0.4280, 0.0000, 0.0695, 0.0000]],\n","\n","         [[0.0000, 0.0000, 0.0000, 0.3407],\n","          [0.3947, 0.0000, 0.5601, 0.0000],\n","          [0.4072, 0.0000, 0.0000, 0.3469],\n","          [0.1545, 0.0000, 0.0000, 0.0310]],\n","\n","         [[0.0088, 0.0000, 0.0000, 0.3615],\n","          [0.0000, 0.1298, 0.0000, 0.0000],\n","          [0.5739, 0.0957, 0.0000, 0.7847],\n","          [0.5605, 0.0000, 0.0000, 0.1339]],\n","\n","         [[0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.4856, 0.0000],\n","          [0.1740, 0.1962, 0.0000, 0.1627],\n","          [0.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<ReluBackward0>)"]},"metadata":{"tags":[]},"execution_count":27}]}]}